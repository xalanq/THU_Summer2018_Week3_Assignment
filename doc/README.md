# Scraper - 爬虫

使用的库有

* requests
* BeautifulSoup4

爬虫分为两部分，网络通信部分（scraper.py）与适配器（adapers/*.py）部分。

## 网络通信部分

网络部分也分为两部分：

* 第一部分是初始化部分，使用适配器提供的链接，下载数据后发给适配器（适配器用这些链接捕获哪些链接是下一步需要爬取的）。
* 第二部分是爬取新闻的部分，适配器在前一步里得到了大量的新闻链接，通信部分便用这些链接进行爬取。爬取之后，再将这些数据传入适配器，然后得到返回值（包含新闻的ID、标题、内容、日期、来源）。

全部爬完之后，将新闻数据以json格式存入到文件里，其中新闻的内容是`html`，不是纯文本（保留了原网站的一些排版、外链图片等信息）。

这一部分是多线程（默认是10个线程）的，也就是说适配器必须要是**线程安全**的。

## 适配器部分

适配器部分为通信部分提供链接（url）、报文头（headers）、请求参数（params），需要实现7个函数

* `hasNextInit()`：判断是否有下一个初始链接，有的话返回True
* `nextInitParam()`：返回下一个初始链接的信息，包括op和上述的url、headers、params，其中op是你想加入的额外的信息
* `init(op, text)`：op表示上一个函数你所加入的额外的信息，text表示上一个函数请求的url所得到的html数据
* `hasNext()`：判断是否有下一个新闻链接，有的话返回True
* `nextParam()`：返回下一个新闻链接的信息，包括op和上述的url、headers、params，其中op是你想加入的额外的信息
* `eval(op, text)`：op表示上一个函数你所加入的额外的信息，text表示上一个函数请求的url所得到的html数据
* 'encoding()'：返回所爬取网页用的编码格式（用于网络部分解析html数据）

请一定注意，这些函数都必须要线程安全。

# Web - 网页

## 前端

* 使用`Boostrap 3`写的UI
* 使用`JavaScript`（大部分是`jQuery`）进行各种UI更新操作，比如分页、高亮、使用`ajax`获取各种服务器上的数据，动态更新网页等
* 包含三种页面：主页（`/`）、搜索页（`/s??wd=中国&bg=2001-01-25&ed=2018-01-25`）、新闻详细页（`/post?id=people_1`）

** 没有抄袭任何他人完整的代码，全是自己写的。**

## 后端

### 新闻的数据存储

我使用的数据库是`Django`默认自带的`SQLite`，因此我只需要实现几个`models`就能实现数据的读写了。我一共写了4个models（位于`/web/postdb/models.py`）：

* `WebInfo`：存储每个适配器（adapter）的数据信息
  * `name`：适配器的名字（比如`people`、`xinhua`）
  * `count`：该适配器目前有多少数据从爬虫部分的json文件里导入进了数据库（用于下一次从该json文件里更新数据）
* `PostInfo`：存储每篇新闻的数据信息
  * `NID`（Number ID）：每篇新闻的纯数字ID（从1开始），用于减少网络通信时数据传输的大小
  * `TID`（Text ID）：每篇新闻的文本ID，是`适配器名字_number`这样命名，比如`people_1`，用于在`/post?id=people_1`里展示（而不是以纯数字的方式，因为这样难以区分）
  * `time`：新闻发表的时间，用`datetime`类型存储
  * `category`：新闻的分类（中文），比如“社会”、“时政”、“军事”等
  * `title`：新闻的标题
  * `content`：新闻的内容（html）
  * `plain`：新闻的内容（纯文本）
  * `url`：新闻是从哪里爬取的？就是从该url爬取的
  * `sourceLink`：新闻的来源链接（每篇新闻都有个来源，不一定就是url）
  * `sourceText`：新闻的来源文本（比如“新华网”、“人民网”）
* `IndexInfo`：存储每个词语对应的新闻（倒排列表索引），同时存储新闻的一些信息
  * `key`：词语
  * `value`：该词语所对应的倒排列表（list），这个列表的每一个元素的格式为`[在该新闻里的出现次数, 该新闻的NID，该新闻的发表时间]`, 比如`[1234, '3', datetime(2018, 1, 2)]`。该列表会转化成json格式的字符串存储在`value`内。
* `PostRelation`：存储每篇新闻相关联的几篇新闻（默认是3篇），将其作为该新闻的推荐新闻
  * `NID`：新闻的`NID`
  * `relation`：相关联新闻的列表（list），这个列表的每一个元素的格式为`{'title': 关联新闻的标题, 'TID': 关联新闻的ITD}`。该列表会转化成json格式的字符串存储在`relation`内。

## 新闻搜索算法

先介绍`IndexInfo`数据库的建立。

将每篇新闻的纯文本进行分词（使用`thulac`），同时统计每个词出现的次数。然后根据格式存入`IndexInfo`里的`value`。

对于每一个搜索的字符串，我们将这个字符串也分词。对于每个词语，我们从`IndexInfo`里取出倒排列表，将每个新闻的出现次数累加。最后根据每条新闻的累加次数，从大到小排个序，然后返回这些新闻的`NID`。

## 推荐新闻算法

用一个最简单的办法：将这篇新闻的标题拿去`新闻搜索算法`里进行搜索，然后取出前几条新闻即可。这是因为，新闻的标题有高度的概括性（而且是人为的），在一定程度上可以代表整篇文章。

我们用该办法预处理一下每篇新闻，然后存入`PostRelation`数据库里即可。



